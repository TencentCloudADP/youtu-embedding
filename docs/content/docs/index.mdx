---
title: Introduction
description: Introduce Youtu-Embedding
---

## üéØ Brief Introduction

**Youtu-Embedding** is an industry-leading, general-purpose text representation model developed by Tencent Youtu Lab. It demonstrates state-of-the-art performance across a wide range of natural language processing tasks, including Information Retrieval (IR), Semantic Textual Similarity (STS), Clustering, Reranking, and Classification.

The core advantages of Youtu-Embedding can be summarized as follows:

- **üèÜ State-of-the-Art Performance**: Achieved a top score of **77.46** on the authoritative Chinese text embedding benchmark CMTEB (as of Sep 2025), proving its powerful representation capabilities.
- **üß† Sophisticated Three-Stage Training:** We pioneered a "LLM-based Pre-training ‚Üí Weakly-supervised Alignment ‚Üí Collaborative-Discriminative Fine-tuning" pipeline, which systematically distills the broad knowledge of large language models into the specialized discriminative power required for embedding tasks.
- **‚≠ê Innovative Fine-tuning Framework**: We designed a unique Collaborative-Discriminative Fine-tuning Framework that effectively resolves the "negative transfer" problem in multi-task learning through a unified data format, task-differentiated loss functions, and a dynamic single-task sampling mechanism. (This framework has been verified on a variety of basic encoders to ensure its versatility and effectiveness.)
- **üõ†Ô∏è Meticulous Data Engineering**: We combined high-quality, LLM-based data synthesis with efficient hard negative mining strategies to provide the most robust data foundation for model training.

## ü§ó Model Download

We have released our first model version on Hugging Face. It is a 2 billion (2B) parameter model designed for general-purpose semantic representation.

| Model                | Parameters | Dimensions | Sequence Length | Hugging Face |
| :------------------- | :--------: | :--------: | :-----------------: | :------------------------------------------------------------------------------------------ |
| Youtu-Embedding-V1   | 2B         | 2048       | 8K               | [Youtu-RAG/Youtu-Embedding-V1](https://huggingface.co/Youtu-RAG/Youtu-Embedding-V1) |


## üöÄ Quick Start & Train Model

import { Cards, Card } from 'fumadocs-ui/components/card';

<Cards>
  <Card
    href="/quickstart"
    title="üöÄ Quick Start Inference"
    description="Generate embeddings via our official API for ease of use or run the model locally for full control."
  />
  <Card
    href="/fine-tuning-framework"
    title="üí° Fine-tuning Framework"
    description="Our novel Collaborative-Discriminative Fine-tuning Framework designed to overcome multi-task optimization challenges."
  />
</Cards>

## üìä CMTEB

Youtu-Embedding demonstrates superior performance across all seven task categories on the CMTEB benchmark and achieves the highest overall average score. We present the results of the latest version of the model as follows:

| Model                     | Mean(Task)         | Mean(Type)         | Class. | Clust. | Pair Class. | Rerank. | Retr.  | STS   |
| :------------------------ | :----------------- | :----------------- | :----: | :----: | :---------: | :-----: | :----: | :---: |
| gte-Qwen2-1.5B-instruct   | 67.12              | 67.79              | 72.53  | 54.61  | 79.50       | 68.21   | 71.86  | 60.05 |
| bge-multilingual-gemma2   | 67.64              | 68.52              | 75.31  | 59.30  | 86.67       | 68.28   | 73.73  | 55.19 |
| ritrieve\_zh\_v1          | 72.71              | 73.85              | 76.88  | 66.50  | 85.98       | 72.86   | 76.97  | 63.92 |
| Qwen3-Embedding-4B        | 72.27              | 73.51              | 75.46  | 77.89  | 83.34       | 66.05   | 77.03  | 61.26 |
| Qwen3-Embedding-8B        | 73.84              | 75.00              | 76.97  | 80.08  | 84.23       | 66.99   | 78.21  | 63.53 |
| Conan-embedding-v2        | 74.24              | 75.99              | 76.47  | 68.84  | 92.44       | 74.41   | 78.31  | 65.48 |
| Seed1.6-embedding         | 75.63              | 76.68              | 77.98  | 73.11  | 88.71       | 71.65   | 79.69  | 68.94 |
| QZhou-Embedding           | 76.99              | 78.58              | 79.99  | 70.91  | 95.07       | 74.85   | 78.80  | 71.89 |
| **Youtu-Embedding-V1-0917** | **77.60** | **78.85** | 78.04 | 79.67 | 89.69 | 73.85 | 80.95 | 70.91 |



## üéâ Citation

If you find our work useful in your research, please consider citing our paper:

```bash
@misc{zhang2025codiemb,
  title={CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity},
  author={Zhang, Bowen and Song, Zixin and Chen, Chunquan and Zhang, Qian-Wen and Yin, Di and Sun, Xing},
  year={2025},
  eprint={2508.11442},
  archivePrefix={arXiv},
  url={https://arxiv.org/abs/2508.11442},
}
```
